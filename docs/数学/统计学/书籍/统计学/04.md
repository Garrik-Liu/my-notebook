# 统计学 (04)

## 简单线性回归

### 回归模型

统计学最重要的应用之一是根据一组有联系的自变量 $x_1, x_2, ..., x_k$ 的认识，估计响应变量 $y$ 的某个未来值, 或者各种相关统计量.

![2020-07-23-21-02-38](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-02-38.png)

🌰 例如, 一个工程师可能想建立 "机械装配线故障率 $y$" 与诸如它的 "运行速度" 和 "装配线操作员" 这样的变量间关系.目的是提出一个联系因变量 $y$ 与自变量的 "预测方程"，并将各种 "运行速度" 和 "操作员" 的组合带入方程中, 来预测故障率 $y$ 的值.

用于联系因变量 $y$ 与自变量 $x_1, x_2, ..., x_k$ 的模型称为『 **回归模型** 』或『 **线性统计模型** 』

统计学理论像物理学、工程学、经济学等理论一样，只是一个现 实的模型. 只有当方法中的假定完全满足时，它才能精确地解释现实. 但现实情况往往是复杂多变的, 很难精准地匹配书本上的理论模型. 所以用统计学解决现实世界问题是一种艺术.

- 为了把理论应用于现实世界，必须知道与假定的背离在多大范围内会影响得到的统计推断;
- 并且需要根据实际问题选取最适当的模型和方法论;

---

![2020-07-23-21-15-43](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-15-43.png)

![2020-07-23-21-15-54](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-15-54.png)

假定认为 $y$ 值随着 $x$ 的增加以线性方式的趋势增加，则在散点图中画一条穿过点的直线作为联系 $y$ 与 $x$ 的模型. 但是上图中我们没办法画出一条穿过所有点的直线.

这时, 我们可以建立一条联系 $y$ 与 $x$ 的『 **概率模型** 』也就是需要承认数据点关于直线的是随机变化的.

概率模型的一种类型是『 **简单线性回归模型** 』, 即假定对给定的 $x$ 值, $y$ 的均值可以绘成一条直线, 并且数据点对这条直线的偏离是一个随机量 $ε$

![2020-07-23-21-31-44](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-31-44.png)

![2020-07-23-21-31-56](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-31-56.png)

为了用简单线性回归模型拟合一组数据, 我们必须求出均值直线中未知参数 $β_0$ 和 $β_1$ 的估计量.

关于 $β_0$ 和 $β_1$ 的有效推断依赖于估计量的抽样分布. 估计量的抽样分布又依赖于随机误差 $ε$ 的概率分布. 对于 $ε$ 我们做出一些假定:

![2020-07-23-21-40-29](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-40-29.png)

![2020-07-23-21-36-23](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-36-23.png)

![2020-07-23-21-38-24](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-38-24.png)

- 上图中描述了 $x$ 的三个特殊值 $x_1, x_2, x_3$ 的误差分布, 可以看出误差的相对频率分布是正态的, 均值为 0, 方差为 $\sigma^2$. 体现出了假定 1, 2, 3。

### 最小二乘法

求 $β_0$ 和 $β_1$ 的最简单方法是『 **最小二乘法** 』

![2020-07-23-21-47-53](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-21-47-53.png)

在图中可以看到每个点到直线有一个垂线, 它代表点到直线的『 **离差 ( 误差 )** 』我们可以找到一条直线, 使得『 **离差 ( 误差 ) 平方和 Sum of squares for error, SSE** 』最小. 这条直线称为『 **最小二乘直线** 』『 **回归直线** 』或『 **最小二乘预测方差** 』

假设我们有 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 分别是 ${\beta_0}$ 和 ${\beta_0}$ 的估计量:

- 那么对于一个给定的数据点 $(x_i, y_i)$ , $y$ 的观测值是 $y_i$
- 把 $x_i$ 带入方程 $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i$ 得到预测值 $\hat{y_i}$
- 则 $y$ 的观测值与它的预测值的离差为 $(y_i - \hat{y_i}) = [y_i - (\hat{\beta_0} + \hat{\beta_1}x_i)]$

对所有 $n$ 个数据点, $y$ 值与预测值的『 离差平方和 』为:

$$SSE = \sum_{i=1}^{n}[y_i - \hat{y_i}]^2 = \sum_{i=1}^{n}[y_i - (\hat{\beta_0} + \hat{\beta_1}x_i)]^2$$

使 SSE 最小的 $\hat{\beta_0}$ 和 $\hat{\beta_1}$ 称为『 **最小二乘估计值** 』那么 $y = \hat{\beta_0} + \hat{\beta_1}x$ 就是『 **最小二乘直线** 』

我们可以通过『 **最小二乘估计公式** 』去算 $\hat{\beta_0}$ 和 $\hat{\beta_1}$

![2020-07-23-23-09-32](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-23-09-32.png)

::: details-open 🌰 例子：

![2020-07-23-23-10-06](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-23-10-06.png)
![2020-07-23-23-10-44](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-23-23-10-44.png)

:::

### 最小二乘估计的性质

### $σ^2$ 的估计量

### 评价模型的效用

### 相关系数

### 决定系数

### 利用模型估计和预测

### 一个完整的例子

![2020-07-26-22-45-05](https://garrik-default-imgs.oss-accelerate.aliyuncs.com/imgs/2020-07-26-22-45-05.png)

## 多重线性回归

## 模型构建

## 试验设计原理
